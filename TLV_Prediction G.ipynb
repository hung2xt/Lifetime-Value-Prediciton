{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CUSTOMER LIFETIME VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normaly, we invest in customers that are often acquisitions costs, online and offline ads, promotions in order to create revenue and be profitable. These actions can create some customers with higher valuable in terms of lifetime, but there are always some customers who reduce the profitablity. The problem is we want to identify the customers' behaviors or say, patterns. This requires a step that we segment customers into different groups and act comparably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to select a time window, it can be anything like 3,6,9,12 or 24 months. We can use a equation below to compute the Lifetime value for every customer in that specific time window:\n",
    "\n",
    "        Lifetime value = Total Gross Revenues - Total Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is as we migth have some customers having very high negative lifetime value historically, it could be too late to take an action. At this point, we need to predict the future with machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some important steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define an appropriate time frame for customer lifetime value\n",
    "2. Determine the features we will use to forecast the future and create them\n",
    "3. Compute lifetime value (LTV) for the purpose of training machine learning model\n",
    "4. Build and excute the machine learning model\n",
    "5. Check if the model is helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the time somehow depends on industry, business model, and strategy and more that vary. Consider for example, 1 year is a very short for some industries while the others it is a very long period. In our case, we will opt the time frame with 6 months ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to calculate RFM score for each customer ID. To implement it correctly, we need to split our dataset. We will take 3 months of data, compute RFM and use this result for forecasting next 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-f6wZ8OkYAY",
    "outputId": "25cdf879-0cce-493f-f4a2-a145b37b9822"
   },
   "outputs": [],
   "source": [
    "#We shall import some libraries\n",
    "from __future__ import division\n",
    "from datetime import datetime, timedelta,date\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as pyoff\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#initate plotly\n",
    "pyoff.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from csv and redo the data work we done before\n",
    "df = pd.read_csv('X_Southeast_Asia_retail.csv')\n",
    "df['InvoiceDate'] = pd.to_datetime(tx_data['InvoiceDate'])\n",
    "df_Vietnam = df.query(\"Country=='Vietnam'\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the 3 months and 6 months dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3m and 6m dataframes\n",
    "df_3months = df_Vietnam[(df_Vietnam.InvoiceDate < date(2018,6,1)) & (df_Vietnam.InvoiceDate >= date(2018,3,1))].reset_index(drop=True)\n",
    "df_6months = df_Vietnam[(df_Vietnam.InvoiceDate >= date(2018,6,1)) & (df_Vietnam.InvoiceDate < date(2018,12,1))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df_user for classifying clustering\n",
    "df_user = pd.DataFrame(df_3months['CustomerID'].unique())\n",
    "df_user.columns = ['CustomerID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a fucntion for ordering cluster numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_cluster(cluster_field_name, target_field_name,df,ascending):\n",
    "    new_cluster_field_name = 'new_' + cluster_field_name\n",
    "    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n",
    "    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n",
    "    df_new['index'] = df_new.index\n",
    "    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n",
    "    df_final = df_final.drop([cluster_field_name],axis=1)\n",
    "    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use KMeans clustering to compute Recency score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate recency score\n",
    "df_max_purchase = df_3months.groupby('CustomerID').InvoiceDate.max().reset_index()\n",
    "df_max_purchase.columns = ['CustomerID','max_purchase_date']\n",
    "df_max_purchase['Recency'] = (tx_max_purchase['max_purchase_date'].max() - tx_max_purchase['max_purchase_date']).dt.days\n",
    "df_user = pd.merge(df_user, df_max_purchase[['CustomerID','Recency']], on='CustomerID')\n",
    "#Initiate the KMeans method\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(df_user[['Recency']])\n",
    "#Predict clusters\n",
    "df_user['recency_cluster'] = kmeans.predict(df_user[['Recency']])\n",
    "df_user = order_cluster('recency_cluster', 'Recency',tx_user,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can use this method to calculate frequency score again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcuate frequency score\n",
    "df_frequency = tx_3m.groupby('CustomerID').InvoiceDate.count().reset_index()\n",
    "df_frequency.columns = ['CustomerID','Frequency']\n",
    "df_user = pd.merge(tx_user, tx_frequency, on='CustomerID')\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(tx_user[['Frequency']])\n",
    "df_user['frequency_cluster'] = kmeans.predict(tx_user[['Frequency']])\n",
    "\n",
    "df_user = order_cluster('frequency_cluster', 'Frequency',tx_user,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualate the revenue score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FR86RyjMkYAl",
    "outputId": "2bc5aaeb-4f50-4bf7-e4f6-c87aab73f33c"
   },
   "outputs": [],
   "source": [
    "#calcuate revenue score\n",
    "df_3months['Revenue'] = df_3months['UnitPrice'] * df_3months['Quantity']\n",
    "df_revenue = df_3months.groupby('CustomerID').Revenue.sum().reset_index()\n",
    "tx_user = pd.merge(df_user, df_revenue, on='CustomerID')\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(df_user[['Revenue']])\n",
    "df_user['revenue_cluster'] = kmeans.predict(df_user[['Revenue']])\n",
    "df_user = order_cluster('revenue_cluster', 'Revenue',tx_user,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating these three scores including recency, requency, revenue score. We are going to create an overall score out of them!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can name these score: 0-2: Low Value, 3-4: Mid Value, >5: High Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall scoring\n",
    "df_user['overall_score'] = df_user['recency_cluster'] + df_user['requency_cluster'] + df_user['revenue_cluster']\n",
    "df_user['Segment'] = 'Low-Value'\n",
    "df_user.loc[df_user['overall_score']>2,'Segment'] = 'Mid-Value' \n",
    "df_user.loc[df_user['overall_score']>4,'Segment'] = 'High-Value' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our features is now ready, we will compute 6 months Lifetime value for each customer ID that are used for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NAM4_pPekYAq",
    "outputId": "1cf9b6a7-b9bd-41c2-a42d-2adc1845f25f"
   },
   "outputs": [],
   "source": [
    "df_6months['Revenue'] = df_6months['UnitPrice'] * df_6months['Quantity']\n",
    "df_user_6m = df_6months.groupby('CustomerID')['Revenue'].sum().reset_index()\n",
    "df_user_6m.columns = ['CustomerID','m6_Revenue']\n",
    "\n",
    "\n",
    "#we can plot LTV histogram\n",
    "plot_data = [go.Histogram(x=df_user_6m.query('m6_Revenue < 10000')['m6_Revenue'])]\n",
    "\n",
    "plot_layout = go.Layout(title='6m Revenue')\n",
    "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
    "pyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this code aove, we will see the insight that we have customers with negative Lifetime value. And there are also some outliers too. Removing out the outliers of this dataset will make sense and have a correct machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we will merge our 3 months and 6 months dataframes to view correlation between LTV and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIxGZtk_kYAw",
    "outputId": "af266975-166f-4c89-fc3c-778546673503"
   },
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_user, df_user_6m, on='CustomerID', how='left')\n",
    "df_merge = df_merge.fillna(0)\n",
    "\n",
    "df_graph = df_merge.query(\"m6_Revenue < 30000\")\n",
    "\n",
    "plot_data = [\n",
    "    go.Scatter(\n",
    "        x=tx_graph.query(\"Segment == 'Low-Value'\")['OverallScore'],\n",
    "        y=tx_graph.query(\"Segment == 'Low-Value'\")['m6_Revenue'],\n",
    "        mode='markers',\n",
    "        name='Low',\n",
    "        marker= dict(size= 7,\n",
    "            line= dict(width=1),\n",
    "            color= 'blue',\n",
    "            opacity= 0.8\n",
    "           )\n",
    "    ),\n",
    "        go.Scatter(\n",
    "        x=tx_graph.query(\"Segment == 'Mid-Value'\")['OverallScore'],\n",
    "        y=tx_graph.query(\"Segment == 'Mid-Value'\")['m6_Revenue'],\n",
    "        mode='markers',\n",
    "        name='Mid',\n",
    "        marker= dict(size= 9,\n",
    "            line= dict(width=1),\n",
    "            color= 'green',\n",
    "            opacity= 0.5\n",
    "           )\n",
    "    ),\n",
    "        go.Scatter(\n",
    "        x=tx_graph.query(\"Segment == 'High-Value'\")['OverallScore'],\n",
    "        y=tx_graph.query(\"Segment == 'High-Value'\")['m6_Revenue'],\n",
    "        mode='markers',\n",
    "        name='High',\n",
    "        marker= dict(size= 11,\n",
    "            line= dict(width=1),\n",
    "            color= 'red',\n",
    "            opacity= 0.9\n",
    "           )\n",
    "    ),\n",
    "]\n",
    "\n",
    "plot_layout = go.Layout(\n",
    "        yaxis= {'title': \"6m LTV\"},\n",
    "        xaxis= {'title': \"RFM Score\"},\n",
    "        title='LTV'\n",
    "    )\n",
    "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
    "pyoff.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we excute the code above, We would observe that positive correlation is quite observable. This gave us insights that high RFM score means high LTV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the machine learning model, we need to determine what is the type of this machine learning problem. LTV itself is a regression problem. A machine learning model can predict the value of the LTV. But here, we want LTV segments. Because this makes it more actionable insights and easy to communicate with non-technical users. By applying K-means clustering, we can explore our existing LTV groups/segmentatations and build segments on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tip here is that we need to classify customers differently based on their predicted LTV. For instance, we will train the model and have 3 segmentations, then we apply these result on our dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply KMeans to decide number of segmentations and view its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DkkbqM2LkYA1",
    "outputId": "90943f83-e3ed-4d88-c501-4c508f650001"
   },
   "outputs": [],
   "source": [
    "#We filter out outliers\n",
    "df_merge = df_merge[df_merge['m6_Revenue']<df_merge['m6_Revenue'].quantile(0.99)]\n",
    "\n",
    "#creating 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(df_merge[['m6_Revenue']])\n",
    "df_merge['ltv_cluster'] = kmeans.predict(df_merge[['m6_Revenue']])\n",
    "\n",
    "#order cluster number based on LTV\n",
    "df_merge = order_cluster('ltv_cluster', 'm6_Revenue',tx_merge,True)\n",
    "\n",
    "#creating a new cluster dataframe\n",
    "df_cluster = df_merge.copy()\n",
    "\n",
    "#we can see the summary of desciptive statistic of each cluster\n",
    "df_cluster.groupby('ltv_cluster')['m6_Revenue'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfNq4OsokYA7"
   },
   "source": [
    "There are a few steps before training the machine learning model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert categorical variables onto numberical variables\n",
    "2. Check the correlation of features versus our label, LTV clusters\n",
    "3. Split the dataset on train and test set\n",
    "4. Run the machine learning model to see its real performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tp3DajDlkYBA"
   },
   "outputs": [],
   "source": [
    "#convert categorical variables to dummy variables\n",
    "df_class = pd.get_dummies(df_cluster, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqGwKDAikYBL"
   },
   "outputs": [],
   "source": [
    "#We can calculate correlation and print them out\n",
    "corr_matrix = df_class.corr()\n",
    "corr_matrix['ltv_cluster'].sort_values(ascending=False)\n",
    "\n",
    "#Create X and y\n",
    "X = df_class.drop(['ltv_cluster','m6_Revenue'], axis=1)\n",
    "y = tx_class['ltv_cluster']\n",
    "\n",
    "#Split train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=56, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we excute this code, we would observe that Revenue, Frequency and RFM score will be useful in our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use XGBoost to classify our customers. It becomes a multiple classification model because we had 3 groups in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOMDORh_kYBP",
    "outputId": "34510996-cce9-4ff9-db81-69fb1d196fb9"
   },
   "outputs": [],
   "source": [
    "LTV_XGBOOST_MODEL = xgb.XGBClassifier(max_depth=5, learning_rate=0.1,\n",
    "                                     objective='multi:softprob', n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "#We will print the accuaracy of XGBoost\n",
    "print(f'Accuray of XGBoost Model on training set = {round(LTV_XGBOOST_MODEL.score(X_train, y_train),2)}')\n",
    "print(f'Accuray of XGBoost Model on testing set = {round(LTV_XGBOOST_MODEL.score(X_test, y_test),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this code, we can see that biggest cluster we have is cluster 0 which is 76.5% of the total.\n",
    "84% vs 76.5% will tell us that our machine learning model is useful or not but needs some improvement for sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify that by looking at classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nho0y5iWkYBU",
    "outputId": "276d8def-2d5f-472c-b99f-18b241270961"
   },
   "outputs": [],
   "source": [
    "y_pred = LTV_Xgboost_Model.predict(X_test)\n",
    "print(f'Classification Report = {classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall are defensible for 0. For example, for cluster 0 (Low LTV), if model says that this customer belongs to cluster 0, 90 out of 100 will be correct. And the model successfully identifies 94% of actual cluster 0 customers (recall). We really need to improve the model for other clusters. For example, we barely detect 60% of Mid LTV customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vkk7CvEBkYBZ"
   },
   "outputs": [],
   "source": [
    "y_pred_LTV = pd.DataFrame({'CustomerID':X_test['CustomerID'],'LTV' 'Prediction':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KCYoGj9kYBd"
   },
   "outputs": [],
   "source": [
    "y_pred_LTV.to_csv('y_pred_LTV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2ZdM6cYkYBi"
   },
   "outputs": [],
   "source": [
    "#prediction_LTV = pd.merge([y_pred_LTV['y_pred'], y_test])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TLV Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
